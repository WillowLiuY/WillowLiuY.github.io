<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Liu "Willow" Yang - Personal Website</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>
    <header>
        <div class="header-content">
            <div class="name">LIU YANG</div>
            <nav>
                <ul>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="research.html">Research</a></li>
                    <li><a href="cv.html">CV</a></li>
                    <li><a href="software.html">Projects</a></li>
                    <li><a href="github.html">Github</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main>
        <section id="home">
            <h1>About Me</h1>
            <div class="content">
                <div class="photo">
                    <img src="images/photo-ly.jpeg" alt="Liu Yang">
                </div>
                <div class="text">
                    <p>Hello! I'm Willow. I am a developer & researcher passionate about the fascinating intersection of computer vision, deep learning, and robotic systems.</p>
                    <p>I hold a PhD from Purdue University, where I was a member of the Lab of Computer-Integrated Infrastructure Informatics (LCIII). My research focused on developing vision algorithms with real-time in-flight vision sensors to enable robotic agents, such as unmanned ground vehicles (UGVs), to perceive the complex environments and make better decisions.</p>
                    <p>Currently, I am continuing this research as a Research Associate at the Department of Mechanical, Aerospace and Biomedical Engineering (MABE) at University of Tennessee, Knoxville (UTK), where I am developing 3D vision-based algorithms for AI-driven laparoscope robots. These algorithms provide more accurate depth perception and patient-specific models when combining multi-source preoperative and intraoperative vision data and anotomy deformation modeling.</p>
                    <p>If you'd like to connect, please don't hesitate to reach out! The best way to get in touch is through <a href="https://www.linkedin.com/feed/">LinkedIn</a>. Let's innovate together and shape the future of intelligent robotics!</p>
</div>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Liu Yang</p>
    </footer>
</body>
</html>
